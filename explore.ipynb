{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"D:\\Datasets\\YouNiverse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_channels = pd.read_csv(data_path + \"\\df_channels_en.tsv.gz\", sep=\"\\t\")\n",
    "df_timeseries = pd.read_csv(data_path + \"\\df_timeseries_en.tsv.gz\", sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_channels.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_timeseries.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_palette = px.colors.qualitative.Set2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 1: Line graph showing the year-on-year growth of gaming channels, videos, and viewership\n",
    "# Preprocess the 'df_timeseries' for year-on-year analysis\n",
    "# Example code for preprocessing\n",
    "df_timeseries['datetime'] = pd.to_datetime(df_timeseries['datetime'])\n",
    "df_timeseries['month_year'] = df_timeseries['datetime'].dt.to_period('Q')\n",
    "\n",
    "# Example aggregation for monthly data\n",
    "monthly_stats = df_timeseries.groupby('month_year').agg({\n",
    "    'channel': 'nunique', \n",
    "    'videos': 'sum', \n",
    "    'views': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Creating the subplots\n",
    "fig1 = make_subplots(rows=1, cols=3, subplot_titles=('Channels', 'Videos', 'Views'))\n",
    "\n",
    "# Adding traces\n",
    "fig1.add_trace(go.Scatter(x=monthly_stats['month_year'].astype(str), y=monthly_stats['channel'], mode='lines', name='Channels'), row=1, col=1)\n",
    "fig1.add_trace(go.Scatter(x=monthly_stats['month_year'].astype(str), y=monthly_stats['videos'], mode='lines', name='Videos'), row=1, col=2)\n",
    "fig1.add_trace(go.Scatter(x=monthly_stats['month_year'].astype(str), y=monthly_stats['views'], mode='lines', name='Views'), row=1, col=3)\n",
    "\n",
    "# Updating layout\n",
    "fig1.update_layout(title='Quarterly Growth of Gaming Channels, Videos, and Viewership on YouTube', showlegend=False)\n",
    "fig1.update_xaxes(title_text='Month-Year', row=3, col=1)\n",
    "fig1.update_yaxes(title_text='Count')\n",
    "\n",
    "# Display the figure\n",
    "# fig1.show()\n",
    "\n",
    "fig1.write_html(\"quarterly_growth_gaming.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 2: Demographic pie charts and engagement bar graphs\n",
    "# For this visualization, assuming 'category_cc' represents demographics and 'subscribers_cc' represents engagement\n",
    "# Calculating demographics and engagement\n",
    "demographics = df_channels['category_cc'].value_counts()\n",
    "engagement = df_channels.groupby('category_cc')['subscribers_cc'].sum()\n",
    "\n",
    "extended_color_palette = px.colors.qualitative.Set2 + px.colors.qualitative.Pastel1 + px.colors.qualitative.Dark2\n",
    "\n",
    "# Trimming or extending the palette to match the number of categories\n",
    "if len(extended_color_palette) > len(demographics.index):\n",
    "    extended_color_palette = extended_color_palette[:len(demographics.index)]\n",
    "elif len(extended_color_palette) < len(demographics.index):\n",
    "    extended_color_palette.extend(px.colors.qualitative.Plotly[len(extended_color_palette) - len(demographics.index):])\n",
    "\n",
    "# Matching the extended, more varied color palette to the categories\n",
    "colors = {category: extended_color_palette[i] for i, category in enumerate(demographics.index)}\n",
    "\n",
    "# Recreating the pie chart and bar graph with the new color palette\n",
    "fig2 = make_subplots(rows=1, cols=2, specs=[[{'type':'domain'}, {'type':'bar'}]])\n",
    "\n",
    "fig2.add_trace(\n",
    "    go.Pie(\n",
    "        labels=demographics.index, \n",
    "        values=demographics.values, \n",
    "        name='Demographics', \n",
    "        marker=dict(colors=[colors[label] for label in demographics.index])\n",
    "    ), \n",
    "    1, 1\n",
    ")\n",
    "\n",
    "fig2.add_trace(\n",
    "    go.Bar(\n",
    "        x=engagement.index, \n",
    "        y=engagement.values, \n",
    "        name='Engagement', \n",
    "        marker=dict(color=[colors[label] for label in engagement.index])\n",
    "    ), \n",
    "    1, 2\n",
    ")\n",
    "\n",
    "fig2.update_layout(title='Demographic Distribution and Engagement in YouTube Gaming')\n",
    "\n",
    "\n",
    "# fig2.show()\n",
    "fig2.write_html(\"demographics_engagement_gaming.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig3 = px.violin(df_channels, y='subscribers_cc', x='category_cc', box=True, points=\"all\",\n",
    "                title='Subscribers Distribution Across Categories (Violin Plot)')\n",
    "# fig3.show()\n",
    "fig3.write_html(\"subscribers_distribution_gaming.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "category_cc\tjoin_date\tchannel\tname_cc\tsubscribers_cc\tvideos_cc\tsubscriber_rank_sb\tweights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "categorical_features = ['category_cc']\n",
    "numerical_features = ['videos_cc', 'weights', 'subscriber_rank_sb']\n",
    "\n",
    "# One-hot encode the categorical variable 'category_cc'\n",
    "one_hot_encoder = OneHotEncoder()\n",
    "category_encoded = one_hot_encoder.fit_transform(df_channels[['category_cc']]).toarray()\n",
    "category_encoded_df = pd.DataFrame(category_encoded, columns=one_hot_encoder.get_feature_names_out(['category_cc']))\n",
    "\n",
    "# Include the encoded categorical data with the numerical features\n",
    "X = pd.concat([df_channels[numerical_features], category_encoded_df], axis=1)\n",
    "y = df_channels['subscribers_cc']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reduce the number of estimators to speed up the process\n",
    "rf_model = RandomForestRegressor(n_estimators=10, random_state=42)\n",
    "\n",
    "# If the dataset is too large, use a fraction of it to train the model\n",
    "if len(X_train) > 100000:  # arbitrary threshold for demonstration purposes\n",
    "    X_train_sample = X_train.sample(n=100000, random_state=42)\n",
    "    y_train_sample = y_train.loc[X_train_sample.index]\n",
    "else:\n",
    "    X_train_sample = X_train\n",
    "    y_train_sample = y_train\n",
    "\n",
    "# Train the model on the smaller sample set\n",
    "rf_model.fit(X_train_sample, y_train_sample)\n",
    "\n",
    "# Predict the number of subscribers on the test set\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "mse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the aesthetic style of the plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Define the figure for the subplot\n",
    "fig, axes = plt.subplots(nrows=5, ncols=2, figsize=(20, 30))\n",
    "\n",
    "# Histogram of subscribers\n",
    "sns.histplot(df_channels['subscribers_cc'], bins=3000, kde=False, ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Distribution of Subscribers')\n",
    "\n",
    "# Boxplot of subscribers across different categories\n",
    "sns.boxplot(x='subscribers_cc', y='category_cc', data=df_channels, ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Subscribers Distribution Across Categories')\n",
    "\n",
    "# Scatter plot of subscribers vs. videos\n",
    "sns.scatterplot(x='videos_cc', y='subscribers_cc', data=df_channels, ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Subscribers vs. Videos')\n",
    "\n",
    "# Correlation heatmap\n",
    "corr = df_channels.corr()\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap='coolwarm', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Correlation Heatmap')\n",
    "\n",
    "# Pairplot for numerical features (sampled due to large size)\n",
    "sns.pairplot(df_channels.sample(min(500, len(df_channels)), random_state=42), diag_kind='kde')\n",
    "\n",
    "# Average number of videos and subscribers in each category\n",
    "category_stats = df_channels.groupby('category_cc').agg({'videos_cc':'mean', 'subscribers_cc':'mean'}).reset_index()\n",
    "sns.barplot(x='videos_cc', y='category_cc', data=category_stats, ax=axes[2, 0])\n",
    "axes[2, 0].set_title('Average Number of Videos per Category')\n",
    "\n",
    "sns.barplot(x='subscribers_cc', y='category_cc', data=category_stats, ax=axes[2, 1])\n",
    "axes[2, 1].set_title('Average Number of Subscribers per Category')\n",
    "\n",
    "\n",
    "\n",
    "# The following codes ensure that the same color is used for the same category across all plots\n",
    "# Filter out NaN values and ensure that all entries are strings\n",
    "filtered_categories = df_channels['category_cc'].dropna().astype(str)\n",
    "\n",
    "# Create a sorted list of unique categories\n",
    "unique_categories = sorted(filtered_categories.unique())\n",
    "\n",
    "# Create a color palette with a distinct color for each category\n",
    "colors = sns.color_palette('Paired', len(unique_categories))\n",
    "\n",
    "# Create a color map (dictionary) for each category\n",
    "color_map = {category: color for category, color in zip(unique_categories, colors)}\n",
    "\n",
    "\n",
    "\n",
    "# Pie chart for proportion of subscribers in each category\n",
    "category_subscribers = df_channels.groupby('category_cc')['subscribers_cc'].sum()\n",
    "axes[3, 0].pie(category_subscribers, labels=category_subscribers.index, autopct='%1.1f%%', colors=[color_map[cat] for cat in category_subscribers.index])\n",
    "axes[3, 0].set_title('Proportion of Subscribers in Each Category')\n",
    "\n",
    "# Pie chart for proportion of channels in each category\n",
    "category_counts = df_channels['category_cc'].value_counts()\n",
    "axes[3, 1].pie(category_counts, labels=category_counts.index, autopct='%1.1f%%', colors=[color_map[cat] for cat in category_counts.index])\n",
    "axes[3, 1].set_title('Proportion of Channels in Each Category')\n",
    "\n",
    "# Violin plot for subscribers across different categories\n",
    "sns.violinplot(x='subscribers_cc', y='category_cc', data=df_channels, ax=axes[4, 0])\n",
    "axes[4, 0].set_title('Subscribers Distribution Across Categories (Violin Plot)')\n",
    "\n",
    "# Stacked bar chart for distribution of videos and subscribers in each category\n",
    "category_grouped = df_channels.groupby('category_cc').agg({'videos_cc':'sum', 'subscribers_cc':'sum'})\n",
    "category_grouped.plot(kind='bar', stacked=True, ax=axes[4, 1])\n",
    "axes[4, 1].set_title('Distribution of Videos and Subscribers in Each Category')\n",
    "\n",
    "# Adjust the layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, it seems that `subscribers` vs `videos` graph follows a power law distribution. Let's check it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Determine the 95th percentile of subscribers\n",
    "percentile_95 = np.percentile(df_channels['subscribers_cc'], 95)\n",
    "\n",
    "# Filter the dataset for channels that have subscriber counts above the 95th percentile\n",
    "upper_bound_data = df_channels[df_channels['subscribers_cc'] >= percentile_95]\n",
    "\n",
    "# Log-transform the upper bound data\n",
    "log_subscribers_upper = np.log(upper_bound_data['subscribers_cc'] + 1)\n",
    "log_videos_upper = np.log(upper_bound_data['videos_cc'] + 1)\n",
    "\n",
    "# Reshape the data for sklearn\n",
    "X_upper = log_videos_upper.values.reshape(-1, 1)\n",
    "y_upper = log_subscribers_upper.values.reshape(-1, 1)\n",
    "\n",
    "# Create and fit the model for the upper bound data\n",
    "model_upper = LinearRegression()\n",
    "model_upper.fit(X_upper, y_upper)\n",
    "\n",
    "# Get the coefficient (k) and the intercept (log(a)) for the upper bound model\n",
    "k_upper = model_upper.coef_[0][0]\n",
    "log_a_upper = model_upper.intercept_[0]\n",
    "\n",
    "# Calculate the R-squared value to assess the fit for the upper bound model\n",
    "r_squared_upper = model_upper.score(X_upper, y_upper)\n",
    "\n",
    "# Convert log(a) back to a for the upper bound model\n",
    "a_upper = np.exp(log_a_upper)\n",
    "\n",
    "# Return the model parameters and R-squared value for the upper bound\n",
    "a_upper, k_upper, r_squared_upper, percentile_95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The exponent $k$ for the upper bound is approximately 0.0617.\n",
    "\n",
    "The $R^2$ value for the upper bound model is approximately 0.0191, which is even lower than the model for the entire dataset. This lower $R^2$ value indicates that the power-law model explains an even smaller portion of the variance within the upper bound of the data.\n",
    "\n",
    "Or, if we want to visualize the data in a log-log plot, we can see that the data points are not linearly distributed. \n",
    "![log-logGraph](\\images\\graph1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_timeseries.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df_timeseries is your time-series data with columns as specified\n",
    "# Scaling the 'subs' column from df_timeseries\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "df_timeseries['scaled_subs'] = scaler.fit_transform(df_timeseries['subs'].values.reshape(-1,1))\n",
    "\n",
    "# We create sequences of data for the LSTM\n",
    "def create_sequences(input_data, time_steps):\n",
    "    inout_seq = []\n",
    "    L = len(input_data)\n",
    "    for i in range(L - time_steps):\n",
    "        train_seq = input_data[i:i + time_steps]\n",
    "        train_label = input_data[i + time_steps:i + time_steps + 1]\n",
    "        inout_seq.append((train_seq, train_label))\n",
    "    return inout_seq\n",
    "\n",
    "time_steps = 5  # For example, using 5 days of data to predict the 6th day\n",
    "df_timeseries_sorted = df_timeseries.sort_values(by=['channel', 'datetime'])\n",
    "sequences = df_timeseries_sorted.groupby('channel')['scaled_subs'].apply(lambda x: create_sequences(x.tolist(), time_steps))\n",
    "sequences = [item for sublist in sequences for item in sublist]  # Flatten the list\n",
    "\n",
    "# Splitting the data into features and targets\n",
    "X, y = zip(*sequences)\n",
    "X = np.array(X).reshape(-1, time_steps, 1)\n",
    "y = np.array(y).reshape(-1, 1)\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "X_tensor = torch.FloatTensor(X)\n",
    "y_tensor = torch.FloatTensor(y)\n",
    "\n",
    "# Create the DataLoader\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "train_loader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# LSTM Model\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_layer_size=100, output_size=1):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_layer_size, output_size)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        lstm_out, _ = self.lstm(input_seq)\n",
    "        predictions = self.linear(lstm_out[:, -1, :])  # Take the last time step only\n",
    "        return predictions\n",
    "\n",
    "# Instantiate the model, define the loss function and the optimizer\n",
    "model = LSTM()\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the model\n",
    "epochs = 5\n",
    "loss_history = []\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for i, (seq, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(seq)\n",
    "        single_loss = loss_function(y_pred, labels)\n",
    "        single_loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_history.append(single_loss.item())\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {single_loss.item():.4f}')\n",
    "\n",
    "    # Optionally, add code to evaluate the model on a validation set here\n",
    "\n",
    "# Visualize the training progress\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(loss_history, label='Training loss')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Include evaluation and other components as needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aspect 1: Predicting Channel Growth Trajectories\n",
    "**Research Question:**\n",
    "Can we predict the future growth of a channel in terms of subscribers and views based on past performance and content category?\n",
    "\n",
    "**Code Interpretation:**\n",
    "You would use time series analysis and machine learning to forecast growth. By applying models like ARIMA for univariate forecasting or LSTM networks for multivariate time series, you can predict future metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assuming df_timeseries is indexed by 'datetime' and each channel's data is a separate time series\n",
    "# Example for one channel:\n",
    "channel_data = df_timeseries[df_timeseries['channel'] == 'UC0UeVA9YHpOEr_Ng442xiRw']\n",
    "\n",
    "# ARIMA model for 'subs'\n",
    "model_subs = ARIMA(channel_data['subs'], order=(5,1,0))\n",
    "model_subs_fit = model_subs.fit()\n",
    "\n",
    "# Forecast the next X steps\n",
    "forecast_subs = model_subs_fit.forecast(steps=10)\n",
    "\n",
    "# Evaluate the model using MSE if you have the actual values to compare with\n",
    "# mse = mean_squared_error(test_data, forecast_subs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aspect 2: Content Strategy Optimization\n",
    "**Research Question:**\n",
    "What content strategies correlate with higher engagement and growth in different categories?\n",
    "\n",
    "**Code Interpretation:**\n",
    "You would perform statistical analysis and machine learning to find patterns and correlations in content strategies across different categories. Regression models or decision trees can reveal what features (like video frequency, length, time of posting) are most predictive of higher engagement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Merge datasets on 'channel' to include category information in df_timeseries\n",
    "merged_data = pd.merge(df_timeseries, df_channels[['channel', 'category']], on='channel')\n",
    "\n",
    "merged_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForest to determine feature importances for 'delta_subs'\n",
    "X = merged_data[['views', 'delta_views', 'videos', 'delta_videos', 'activity']]  # Add more features as needed\n",
    "y = merged_data['delta_subs']\n",
    "\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Feature importances can give insight into which aspects are most predictive of subscriber changes\n",
    "importances = model.feature_importances_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aspect 3: Community Health and Lifecycle Analysis\n",
    "**Research Question:**\n",
    "How does the lifecycle stage of a channel (e.g., growing, mature, declining) affect community health metrics like engagement and sentiment?\n",
    "\n",
    "**Code Interpretation:**\n",
    "You would use clustering to identify lifecycle stages and then analyze how these stages correlate with engagement metrics. Sentiment analysis on video titles/descriptions can be paired with engagement metrics to assess community health."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from textblob import TextBlob\n",
    "\n",
    "# KMeans clustering to identify lifecycle stages based on growth metrics\n",
    "lifecycle_features = df_channels[['subscribers_cc', 'videos_cc']]  # Add other relevant features\n",
    "kmeans = KMeans(n_clusters=3)  # Assuming three lifecycle stages\n",
    "df_channels['lifecycle_stage'] = kmeans.fit_predict(lifecycle_features)\n",
    "\n",
    "# Sentiment analysis on video titles (requires text data)\n",
    "# df_timeseries['title_sentiment'] = df_timeseries['title'].apply(lambda title: TextBlob(title).sentiment.polarity)\n",
    "\n",
    "# Analysis of sentiment vs engagement\n",
    "# merged_data = pd.merge(df_timeseries, df_channels[['channel', 'lifecycle_stage']], on='channel')\n",
    "# lifecycle_sentiment = merged_data.groupby('lifecycle_stage')['title_sentiment'].mean()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
