{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import gzip\n",
    "import swifter\n",
    "import langdetect\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import zstandard as zstd\n",
    "import matplotlib as mpl\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.font_manager as font_manager\n",
    "\n",
    "class zreader:\n",
    "\n",
    "    def __init__(self, file, chunk_size=16384):\n",
    "        self.fh = open(file, 'rb')\n",
    "        self.chunk_size = chunk_size\n",
    "        self.dctx = zstd.ZstdDecompressor()\n",
    "        self.reader = self.dctx.stream_reader(self.fh)\n",
    "        self.buffer = ''\n",
    "\n",
    "    def readlines(self):\n",
    "        while True:\n",
    "            chunk = self.reader.read(self.chunk_size).decode(\"utf-8\", errors=\"ignore\")\n",
    "            if not chunk:\n",
    "                break\n",
    "            lines = (self.buffer + chunk).split(\"\\n\")\n",
    "\n",
    "            for line in lines[:-1]:\n",
    "                yield line\n",
    "\n",
    "            self.buffer = lines[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths: change here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = \"/dlabdata1/youtube_large/\"\n",
    "PATH_TIME_SERIES_SRC = DIR + \"_raw_df_timeseries.tsv.gz\"\n",
    "PATH_TIME_SERIES_DST = DIR + \"df_timeseries_en.tsv.gz\"\n",
    "PATH_CHANNELS_SRC = DIR + \"_raw_df_channels.tsv.gz\"\n",
    "PATH_CHANNELS_DST = DIR + \"df_channels_en.tsv.gz\"\n",
    "PATH_INVALID = DIR + \"invalid.csv\"\n",
    "PATH_METADATA_SRC = DIR + \"_raw_yt_metadata.jsonl.zst\"\n",
    "PATH_METADATA_NON_ENGLISH = DIR + \"yt_metadata_fil.jsonl.gz\"\n",
    "PATH_METADATA_DEDUP = DIR + \"yt_metadata_en_dd/{}.jsonl.gz\"\n",
    "PATH_METADATA_DST = DIR + \"yt_metadata_en.jsonl.gz\"\n",
    "PATH_METADATA_HELPER = DIR + \"yt_metadata_helper.feather\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filters non-english channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = zreader(PATH_METADATA_SRC, chunk_size=16384)\n",
    "dict_channels = {}\n",
    "done_stack = set()\n",
    "idx = 0\n",
    "\n",
    "for line in reader.readlines():\n",
    "    line_dict = json.loads(line)\n",
    "    if line_dict[\"channel_id\"] in done_stack:\n",
    "        continue\n",
    "    tmp = dict_channels.get(line_dict[\"channel_id\"], [])\n",
    "\n",
    "    if len(tmp) == 10:\n",
    "        try:\n",
    "            l = langdetect.detect_langs(\" \".join(tmp))\n",
    "        except:\n",
    "            l = None\n",
    "        dict_channels[line_dict[\"channel_id\"]] = l\n",
    "        done_stack.add(line_dict[\"channel_id\"])\n",
    "    else:\n",
    "        tmp.append(line_dict[\"title\"] + line_dict[\"description\"])\n",
    "        dict_channels[line_dict[\"channel_id\"]] = tmp\n",
    "    idx += 1\n",
    "\n",
    "    if idx % 100000 == 0:\n",
    "        print(idx)\n",
    "\n",
    "print(\"Done:\", len(done_stack), \"Remaining:\", len(dict_channels.keys()))\n",
    "\n",
    "remainder_stack = set(dict_channels.keys()) - done_stack\n",
    "for key_tmp in remainder_stack:\n",
    "    tmp = dict_channels.get(key_tmp, [])\n",
    "    try:\n",
    "        l = langdetect.detect_langs(\" \".join(tmp))\n",
    "    except:\n",
    "        l = None\n",
    "    dict_channels[key_tmp] = l\n",
    "    done_stack.add(key_tmp)\n",
    "\n",
    "print(\"Done:\", len(done_stack), \"Remaining:\", len(dict_channels.keys()))\n",
    "\n",
    "th = 0.6\n",
    "to_remove = set()\n",
    "for key, item in dict_channels.items():\n",
    "    if item is None:\n",
    "        to_remove.add(key)\n",
    "        continue\n",
    "\n",
    "    tmp = [v for v in item if v.lang == \"en\"]\n",
    "    if len(tmp) != 1:\n",
    "        to_remove.add(key)\n",
    "    else:\n",
    "        if tmp[0].prob < th:\n",
    "            to_remove.add(key)\n",
    "\n",
    "print(\"% to remove\", len(to_remove) / len(dict_channels.keys()))\n",
    "df = pd.DataFrame({\"invalid\": list(to_remove)})\n",
    "df.to_csv(PATH_INVALID, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filters time-series and channels files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads stuff\n",
    "df_sb_f = pd.read_csv(PATH_TIME_SERIES_SRC, sep=\"\\t\", compression=\"infer\")\n",
    "df_ch = pd.read_csv(PATH_CHANNELS_SRC, sep=\"\\t\", compression=\"infer\")\n",
    "df_invalid = pd.read_csv(PATH_INVALID)\n",
    "\n",
    "# Filters invalid\n",
    "df_invalid = df_invalid.set_index(\"invalid\")\n",
    "df_invalid[\"val\"] = True\n",
    "dict_channels = dict(df_invalid[\"val\"])\n",
    "df_ch_en = df_ch.loc[df_ch.channel.swifter.apply(lambda x: x not in dict_channels)]\n",
    "df_sb_f_en = df_sb_f.loc[df_sb_f.channel.swifter.apply(lambda x: x not in dict_channels)]\n",
    "\n",
    "# Saves stuff\n",
    "df_ch_en.to_csv(PATH_CHANNELS_DST, sep=\"\\t\", index=False, compression=\"infer\")\n",
    "df_sb_f_en.to_csv(PATH_TIME_SERIES_DST, sep=\"\\t\", index=False, compression=\"infer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filters non-english channels from channel metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = zreader(PATH_METADATA_SRC, chunk_size=16384)\n",
    "writer = gzip.open(PATH_METADATA_NON_ENGLISH, mode=\"wb\")\n",
    "idx = 0\n",
    "\n",
    "valid_channels = set(df_ch_en.channel.values)\n",
    "min_crawl_date, max_crawl_date = pd.to_datetime(\"01-01-2100\"), pd.to_datetime(\"01-01-1990\")\n",
    "min_upload_date, max_upload_date = pd.to_datetime(\"01-01-2100\"), pd.to_datetime(\"01-01-1990\")\n",
    "\n",
    "# Read each line from the reader\n",
    "for line in reader.readlines():\n",
    "    line_dict = json.loads(line)\n",
    "    if line_dict[\"channel_id\"] in valid_channels:\n",
    "        writer.write((json.dumps(line_dict) + \"\\n\").encode())\n",
    "        idx += 1\n",
    "        if pd.to_datetime(line_dict[\"crawl_date\"]) < min_crawl_date:\n",
    "            min_crawl_date = pd.to_datetime(line_dict[\"crawl_date\"])\n",
    "        if pd.to_datetime(line_dict[\"crawl_date\"]) > max_crawl_date:\n",
    "            max_crawl_date = pd.to_datetime(line_dict[\"crawl_date\"])\n",
    "        if pd.to_datetime(line_dict[\"upload_date\"]) < min_upload_date:\n",
    "            min_upload_date = pd.to_datetime(line_dict[\"upload_date\"])\n",
    "        if pd.to_datetime(line_dict[\"upload_date\"]) > max_upload_date:\n",
    "            max_upload_date = pd.to_datetime(line_dict[\"upload_date\"])\n",
    "    if idx % 1000000 == 0:\n",
    "        print(idx)\n",
    "\n",
    "print(idx)\n",
    "print(min_crawl_date, max_crawl_date)\n",
    "print(min_upload_date, max_upload_date)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## De-duplicates channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen = set()\n",
    "expected = 0\n",
    "for df_json in pd.read_json(PATH_METADATA_NON_ENGLISH, compression=\"infer\", chunksize=500000, lines=True):\n",
    "    expected += 500000\n",
    "    print(\"---\", expected)\n",
    "    print(len(df_json), len(seen) / expected)\n",
    "    df_json = df_json.drop_duplicates(\"display_id\")  # drop dups\n",
    "    df_json = df_json.loc[df_json.display_id.apply(lambda x: x not in seen)]  # drops dups from before\n",
    "    seen = set(df_json.display_id)  # uodates before\n",
    "    df_json.to_json(PATH_METADATA_DEDUP.format(str(expected)), lines=True, orient=\"records\", compression=\"infer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate files with zcat !\n",
    "\n",
    "    for f in *.jsonl.gz; do (zcat \"${f}\"; echo) | gzip >> yt_metadata_en.jsonl.gz; done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creates .feather helper without \"description\", \"tags\", \"title\", and \"crawl_date\" fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for df_json in pd.read_json(PATH_METADATA_DST, compression=\"infer\", chunksize=5000000, lines=True):\n",
    "    df_json.drop([\"description\", \"tags\", \"title\", \"crawl_date\"], inplace=True, axis=1)\n",
    "    df_json[\"upload_date\"] = pd.to_datetime(df_json[\"upload_date\"])\n",
    "\n",
    "    dfs.append(df_json)\n",
    "df_final = pd.concat(dfs)\n",
    "df_final.to_feather(PATH_METADATA_HELPER)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creates weigths derived from rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_ch_f = pd.read_csv(PATH_CHANNELS_DST, sep=\"\\t\", compression=\"infer\")\n",
    "\n",
    "def get_dist_rank(df_ch_f, wsize=50, rank_name=\"subscriber_rank\"):\n",
    "    df_ch_f = df_ch_f.sort_values(rank_name).reset_index(drop=True)\n",
    "    ranknonna = df_ch_f[~df_ch_f[rank_name].isna()]\n",
    "    len_ranks = len(ranknonna)\n",
    "    weights, pvalues = [], []\n",
    "\n",
    "    for idx, rank in enumerate(ranknonna.index):\n",
    "        \n",
    "        # first wsize//2 items\n",
    "        if idx < wsize//2:\n",
    "            minv = ranknonna[rank_name].values[0]\n",
    "            maxv = ranknonna[rank_name].values[wsize]\n",
    "            observed = [int(v) for v in ranknonna[rank_name].values[0:wsize]]\n",
    "\n",
    "        # last wsize//2 items\n",
    "        elif idx >= len_ranks - wsize//2:\n",
    "            minv = ranknonna[rank_name].values[-wsize]\n",
    "            maxv = ranknonna[rank_name].values[-1]\n",
    "            observed = [int(v) for v in ranknonna[rank_name].values[-wsize:-1]]\n",
    "        else:\n",
    "            minv = ranknonna[rank_name].values[idx-wsize//2]\n",
    "            maxv = ranknonna[rank_name].values[idx+wsize//2]\n",
    "            observed = [int(v) for v in ranknonna[rank_name].values[idx-wsize//2:idx+wsize//2]]\n",
    "\n",
    "        all_vals = set(list(range(int(minv), int(maxv+1))))\n",
    "        non_observed = np.array(list(all_vals.difference(set(observed))))\n",
    "        observed = np.array(observed)\n",
    "        all_vals = np.zeros(len(all_vals))\n",
    "        all_vals[observed - int(minv)] = 1\n",
    "        p = wsize/(maxv - minv)     \n",
    "        weights.append(p)\n",
    "\n",
    "    weights = 1/np.array(weights)\n",
    "    name_field = \"{}_{}\".format(rank_name, str(wsize))\n",
    "    df_ch_f[\"weights\" + name_field] = np.NaN\n",
    "    df_ch_f.loc[~df_ch_f[rank_name].isna(), \"weights\" + name_field] = weights\n",
    "    return df_ch_f\n",
    "\n",
    "for wsize in [100, 2000, 32000]:\n",
    "    print(wsize)wd\n",
    "    df_ch_f = get_dist_rank(df_ch_f, wsize=wsize, rank_name=\"subscriber_rank_sb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting config\n",
    "fontpath = os.path.expanduser('~/.local/share/fonts/LinLibertine_DRah.ttf')\n",
    "prop = font_manager.FontProperties(fname=fontpath)\n",
    "\n",
    "params = {\n",
    "    \"axes.titlesize\" : 14,\n",
    "    'axes.labelsize': 12,\n",
    "    'font.size': 12,\n",
    "    'legend.fontsize': 12,\n",
    "    'xtick.labelsize': 12,\n",
    "    'ytick.labelsize': 12,\n",
    "    'font.family': prop.get_name(),\n",
    "    'text.usetex': True\n",
    "}\n",
    "\n",
    "mpl.rcParams.update(params)\n",
    "\n",
    "import sys\n",
    "# Local Modules\n",
    "sys.path.insert(0, os.path.abspath('/scratch/horta/youtube_dataset/'))\n",
    "from helpers.plot import set_size\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, figsize=(7, 6), sharex=False, gridspec_kw={\"hspace\": 0.4})\n",
    "\n",
    "vs = [\"#1b9e77\", \"#7570b3\", \"#e6ab02\"]\n",
    "label = [\"$k=100$\", \"$k=2000$\", \"$k=32000$\"]\n",
    "kwargs = {'cumulative': True}\n",
    "\n",
    "ax = axs[0]\n",
    "for idx, wsize in enumerate([100, 2000, 32000]):\n",
    "    ax.plot(df_ch_f[\"subscriber_rank_sb\"].values,\n",
    "            df_ch_f[\"weightssubscriber_rank_sb_{}\".format(str(wsize))],\n",
    "            color=vs[idx], \n",
    "            label=label[idx]\n",
    "           )\n",
    "\n",
    "    x = df_ch_f[\"weightssubscriber_rank_sb_{}\".format(str(wsize))].values\n",
    "    x = x[1:] - x[:-1]\n",
    "    ax.xaxis.grid(color=\"#CCCCCC\", ls=\":\")\n",
    "    ax.yaxis.grid(color=\"#CCCCCC\", ls=\":\")\n",
    "\n",
    "ax.set_title(\"(a) Weights for different window sizes\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend()\n",
    "ax.set_ylabel(\"Weights\")\n",
    "ax.set_xlabel(\"Subscriber Ranking\")\n",
    "\n",
    "ax = axs[1]\n",
    "ax.set_title(\"(b) Adjustment example\")\n",
    "ax.plot(df_ch_f.subscriber_rank_sb,\n",
    "        df_ch_f.videos_cc.rolling(1000).mean(), color=\"gray\", label=\"Moving average over number of videos\")\n",
    "ax.set_xlabel(\"Subscriber Ranking\")\n",
    "ax.axhline(np.mean(df_ch_f.videos_cc), ls=\":\", color=\"black\", label=\"Unadjusted mean $\\mu = 699.78$\")\n",
    "ax.axhline(np.average(df_ch_f.videos_cc, weights=df_ch_f.weightssubscriber_rank_sb_2000), \n",
    "           ls=\"-\", color=\"#7570b3\", label=\"Mean adjusted with weights $\\mu = 559.26$\")\n",
    "ax.legend()\n",
    "ax.xaxis.grid(color=\"#CCCCCC\", ls=\":\")\n",
    "ax.yaxis.grid(color=\"#CCCCCC\", ls=\":\")\n",
    "ax.set_ylabel(\"Number of videos\")\n",
    "set_size(fig, size=(7, 6))\n",
    "fig.savefig(\"./images/weights_window.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saves dataframe with ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ch_f.rename({\"weightssubscriber_rank_sb_2000\": \"weights\"}, axis=1)\\\n",
    "       .drop(['weightssubscriber_rank_sb_100', 'weightssubscriber_rank_sb_32000'], axis=1)\\\n",
    "       .to_csv(PATH_CHANNELS_DST, sep=\"\\t\", compression=\"infer\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}